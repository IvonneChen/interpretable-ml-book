## Interpretability of Neural Networks

TODO: Very short description what a NN is, what CNNs, RNNs, RL are. And what deep learning is.
TODO: Images for each.

Neural networks are a big family of machine learning methods.
A lot of research on neural network interpretability is going on, but it is a quite young field.

MAYBE useful: distinguish during-fit and post-fit methods.


### Looking at components, but not changing them

#### Partial occlusion
Not necessarily model-specific
https://medium.com/merantix/picasso-a-free-open-source-visualizer-for-cnns-d8ed3a35cfc5
TODO: Check if it fits in here

#### Looking at live activations
@Yosinski2015
https://github.com/yosinski/deep-visualization-toolbox

#### Optimizing images
See google cat thing.


#### Analysing the gradient


### Modifying the neural networks somewhat

#### Enforcing monotonicity
Making them monotonic: @Sill

#### Including attention mechanisms

https://www.oreilly.com/ideas/interpretability-via-attentional-and-memory-based-interfaces-using-tensorflow

#### Tweaking the loss function
@Ross2017 in the paper "Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations".
Their approach is to penalize the loss function of gradient based models (read: neural networks) to influence the decision boundaries.
The result is both explaining predictions and regularizing the networks.

Some examples from the paper and comparisons with the LIME method (also described in book Chapter \@ref(lime)) are shown in Figure \@ref(fig:right-for-the-right-reasons-1) and \@ref(fig:right-for-the-right-reasons-2).

```{r right-for-the-right-reasons-1, fig.cap = 'Example from "Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations"'}
knitr::include_graphics("images/ross-2017-1.png")
```

```{r right-for-the-right-reasons-2, fig.cap = 'Another example from "Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations"'}
knitr::include_graphics("images/ross-2017-2.png")
```

### New architectures

#### Attention based

#### Generator and encoder
@Lei2016 propose in their paper "Rationalizing neural predictions" an architecture consisting of a generator and an encoder, which are trained in unison.
The goal is text classification and generate short and coherent text pieces from the target text as rationales for explaining the prediction.
The generator specifies a distribution over the text as candidate rationales and these are passed through the encoder for prediction.
The short text pieces (aka rationales) are highlighted to explain the prediction.
The training method is powered by reinforcement learning and the authors state that encoder and generator can be chosen by the user.
Figure \@ref(fig:rationalizing-neural-predictions) shows an example how such a text highlight looks like.

The code is available at https://github.com/taolei87/rcnn.

```{r rationalizing-neural-predictions, fig.cap = 'Example from "Rationalizing Neural Predictions"'}
knitr::include_graphics("images/rationalizing-neural-predictions.png")
```


### Use MORE neural networks!!!



#### Tweaking the loss function of CNN, using RNN to create sentence

paper: Generating Visual Explanations from @hendricks2016generating.
for classification, new loss function based on sampling and reeinforcement learning generates sentence
TODO: Use image from paper.


#### create explanations of action of RL RNN
The paper "Rationalization: A Neural Machine Translation Approach to Generating Natural Language Explanations" from @harrison2017rationalization presents an approach that uses neural machine translation to translate the inner state of an autonomous agent into natural language.
The training data for these explanations come from players thinking out loud as they play the game.
Really cool idea!
See Figure \@ref(fig:frogger) for an example.


```{r frogger, fig.cap = 'Example from "Rationalization: A Neural Machine Translation Approach to Generating Natural Language Explanations"'}
knitr::include_graphics("images/frogger.png")
```
