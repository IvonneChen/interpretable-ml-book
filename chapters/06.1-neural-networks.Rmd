## Interpretability of Neural Networks

TODO: Very short description what a NN is, what CNNs, RNNs, RL are. And what deep learning is.
TODO: Images for each.

Neural networks are a big family of machine learning methods.
A lot of research on neural network interpretability is going on, but it is a quite young field.

MAYBE useful: distinguish during-fit and post-fit methods.


### Looking at components, but not changing them

#### Partial occlusion
Not necessarily model-specific
https://medium.com/merantix/picasso-a-free-open-source-visualizer-for-cnns-d8ed3a35cfc5
TODO: Check if it fits in here

#### Looking at live activations
@Yosinski2015
https://github.com/yosinski/deep-visualization-toolbox

#### Optimizing images
See google cat thing.


#### Analysing the gradient


### Modifying the neural networks somewhat

#### Enforcing monotonicity
Making them monotonic: @Sill

#### Including attention mechanisms

https://www.oreilly.com/ideas/interpretability-via-attentional-and-memory-based-interfaces-using-tensorflow

#### Tweaking the loss function
@Ross2017 in the paper "Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations".
Their approach is to penalize the loss function of gradient based models (read: neural networks) to influence the decision boundaries.
The result is both explaining predictions and regularizing the networks.

TODO: Add a good paper figure
TODO: Continue description

### New architectures

#### Attention based

#### Generator and encoder
@Lei2016 propose in their paper "Rationalizing neural predictions" an architecture consisting of a generator and an encoder, which are trained in unison.
The goal is text classification and generate short and coherent text pieces from the target text as rationales for explaining the prediction.
The generator specifies a distribution over the text as candidate rationales and these are passed through the encoder for prediction.
The short text pieces (aka rationales) are highlighted to explain the prediction.
The training method is powered by reinforcement learning and the authors state that encoder and generator can be chosen by the user.
Figure \@ref(fig:rationalizing-neural-predictions) shows an example how such a text highlight looks like.

The code is available at https://github.com/taolei87/rcnn.

```{rationalizing-neural-predictions, fig.cap = 'Example from "Rationalizing Neural Predictions"'}
knitr::include_graphics("images/rationalizing-neural-predictions.png")
```


### Use MORE neural networks!!!



#### Tweaking the loss function of CNN, using RNN to create sentence

paper: Generating Visual Explanations from @hendricks2016generating.
for classification, new loss function based on sampling and reeinforcement learning generates sentence
TODO: Use image from paper.


#### create explanations of action of RL RNN
Rationalization: A Neural Machine Translation Approach to Generating Natural Language Explanations
@harrison2017rationalization
Translate internal state-action representations into natural, human language.
TODO: Use image from paper with frog
